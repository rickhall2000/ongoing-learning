{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b646578",
   "metadata": {},
   "source": [
    "Dimensionality Reduction cuases some information loss, so its increase in speed does come at the cost of some loss of performance. \n",
    "\n",
    "It is also useful for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06201de",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0647d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Singular Value Decomposition finds unit vectors\n",
    "\n",
    "X = [] # small 3D dataset\n",
    "#X_centered = X- X.mean(axis=0)\n",
    "#U, s, Vt = np.inalg.svd(X_centered)\n",
    "#c1 = Vt[0]\n",
    "#c2 = Vt[1]\n",
    "\n",
    "# To transform dataset, Multiply X by the number of factors you want to keep\n",
    "# W2 = Vt[:2].T\n",
    "#X2D = X_centered @ W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b18aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "#X2D = pca.fit_transform(X)\n",
    "\n",
    "# You can see the amount of variation that lies among each principal component\n",
    "#pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd18826",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1fb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24f4242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 154\n"
     ]
    }
   ],
   "source": [
    "# could then do this:\n",
    "pca = PCA(n_components=d)\n",
    "# But you could just do this in the first place\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "print(pca.n_components_, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "899345c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('pca', PCA(random_state=42)),\n",
       "                                             ('randomforestclassifier',\n",
       "                                              RandomForestClassifier(random_state=42))]),\n",
       "                   param_distributions={'pca__n_components': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
       "       6...\n",
       "       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
       "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
       "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
       "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "       492, 493, 494, 495, 496, 497, 498, 499])},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also use n_components as a hyperparameter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(PCA(random_state=42),\n",
    "                   RandomForestClassifier(random_state=42))\n",
    "param_distrib = {\n",
    "    \"pca__n_components\": np.arange(10,80),\n",
    "    \"randomforestclassifier__n_estimators\": np.arange(50,500)\n",
    "}\n",
    "rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3, random_state=42)\n",
    "rnd_search.fit(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a9cc9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestclassifier__n_estimators': 465, 'pca__n_components': 23}\n"
     ]
    }
   ],
   "source": [
    "print(rnd_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84acb1a9",
   "metadata": {},
   "source": [
    "### PCA for Compression\n",
    "After reducing to components, you can reverse the operation to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4c29ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e80215",
   "metadata": {},
   "source": [
    "### Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e73a3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cce579",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "Proceeding implementations required whole training set to fit in memory. Incremental PCA allow splitting training set into mini-batches. Useful for large training sets, or applying PCA online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d282cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# Split training set into small batches\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b6d1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use memmap to read the set from disk\n",
    "filename = \"my_mnist.mmap\"\n",
    "X_mmap = np.memmap(filename, dtype=\"float32\", mode=\"write\", shape=X_train.shape)\n",
    "X_mmap[:] = X_train\n",
    "X_mmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20642a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncrementalPCA(batch_size=60000, n_components=154)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mmap = np.memmap(filename, dtype=\"float32\", mode=\"readonly\").reshape(-1, 784)\n",
    "batch_size = X_mmap.shape[0]\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30687caa",
   "metadata": {},
   "source": [
    "## Random Projection\n",
    "PCA gets really slow for large datasets. Random Projection may be needed instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af28cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of dimensions to reduce down to is given by johnson_indenstrauss_min_dim\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "m, epsilon = 5_000, 0.1 # epsilon is the max you are willing for the squared distance between 2 points to change\n",
    "d = johnson_lindenstrauss_min_dim(m, eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74b2e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20_000\n",
    "np.random.seed(42)\n",
    "P = np.random.randn(d, n) / np.sqrt(d)\n",
    "\n",
    "X = np.random.randn(m, n)\n",
    "X_reduced = X @ P.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf9c17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "gaussian_rnd_proj = GaussianRandomProjection(eps=epsilon, random_state=42)\n",
    "X_reduced = gaussian_rnd_proj.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db710d7a",
   "metadata": {},
   "source": [
    "Can use SprseRandomProjection to create a sparse random matrix, meaning smaller memory footpring (25 MB vs 1.2 GB in this example). Is faster to generate and transform (50% in this case). If input is sparse, so will output be, unless you specify dence_output=True. It provides comparable quality, so generally it should be prefered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1006c8aa",
   "metadata": {},
   "source": [
    "## LLE\n",
    "Manifold technique that does not work by projection. Measures how each training instance relates to its nearest neighbor and then looks for low-demiensional represntation that preserves these relationships. Good at unrolling twisted manifolds, particularly when there is not much noise.\n",
    "\n",
    "Scales poorly to large datasets because one step os O(d * m^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89147a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_unrolled = lle.fit_transform(X_swiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0164c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
