{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "72b6affa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from pathlib import Path"
      ],
      "id": "72b6affa"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ff3971d9"
      },
      "outputs": [],
      "source": [
        "# download the data\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
      ],
      "id": "ff3971d9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "91846663"
      },
      "outputs": [],
      "source": [
        "# get the english and spanish lists\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ],
      "id": "91846663"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "21f815b1"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ],
      "id": "21f815b1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0d858ab7"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "id": "0d858ab7"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dcc58b48"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "id": "dcc58b48"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ff242a7c"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ],
      "id": "ff242a7c"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d6913609"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0\n",
        "        p, i = np.meshgrid(np.arange(max_length), \n",
        "                          2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** ( i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]"
      ],
      "id": "d6913609"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "28fe163a"
      },
      "outputs": [],
      "source": [
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)"
      ],
      "id": "28fe163a"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "63724834"
      },
      "outputs": [],
      "source": [
        "N = 2\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = 128\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "encoder_attn_layers = []"
      ],
      "id": "63724834"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3a41a6b5"
      },
      "outputs": [],
      "source": [
        "Z = encoder_in # start with the encoded embeddings\n",
        "\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads = num_heads, key_dim=embed_size, dropout=dropout_rate\n",
        "    )\n",
        "    Z, attn_scores = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask, \n",
        "                                return_attention_scores=True)\n",
        "    encoder_attn_layers.append(attn_scores)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ],
      "id": "3a41a6b5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6cd4c261"
      },
      "outputs": [],
      "source": [
        "batch_max_len_desc = max_length\n",
        "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
        "causal_mask = tf.linalg.band_part( # create lower triangluarl matrix\n",
        "    tf.ones((batch_max_len_desc, batch_max_len_desc), tf.bool), -1, 0\n",
        ")"
      ],
      "id": "6cd4c261"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a7420051"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = Z # This is the result of the encoder layer\n",
        "Z = decoder_in # Now we start Z over for the decoder \n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask) \n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip=Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ],
      "id": "a7420051"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "03c112c2"
      },
      "outputs": [],
      "source": [
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                      outputs=[Y_proba])"
      ],
      "id": "03c112c2"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1e24b41a",
        "outputId": "ea64b89d-60e4-45b8-f604-5fdbac8b34d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " text_vectorization_1 (TextVect  (None, 50)          0           ['input_2[0][0]']                \n",
            " orization)                                                                                       \n",
            "                                                                                                  \n",
            " text_vectorization (TextVector  (None, 50)          0           ['input_1[0][0]']                \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 50, 128)      128000      ['text_vectorization_1[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.not_equal (TFOpLambda)  (None, 50)          0           ['text_vectorization[0][0]']     \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 50, 128)      128000      ['text_vectorization[0][0]']     \n",
            "                                                                                                  \n",
            " positional_encoding (Positiona  (None, 50, 128)     0           ['embedding[0][0]',              \n",
            " lEncoding)                                                       'embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 1, 50)       0           ['tf.math.not_equal[0][0]']      \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  ((None, 50, 128),   527488      ['positional_encoding[0][0]',    \n",
            " dAttention)                     (None, 8, 50, 50))               'tf.__operators__.getitem[0][0]'\n",
            "                                                                 , 'positional_encoding[0][0]']   \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 50, 128)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'positional_encoding[0][0]']    \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 50, 128)     256         ['add[0][0]']                    \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 50, 128)      16512       ['layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 50, 128)      0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 50, 128)      0           ['dropout[0][0]',                \n",
            "                                                                  'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 50, 128)     256         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  ((None, 50, 128),   527488      ['layer_normalization_1[0][0]',  \n",
            " eadAttention)                   (None, 8, 50, 50))               'tf.__operators__.getitem[0][0]'\n",
            "                                                                 , 'layer_normalization_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.math.not_equal_1 (TFOpLambd  (None, 50)          0           ['text_vectorization_1[0][0]']   \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 50, 128)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 1, 50)       0           ['tf.math.not_equal_1[0][0]']    \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 50, 128)     256         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.math.logical_and (TFOpLambd  (None, 50, 50)      0           ['tf.__operators__.getitem_1[0][0\n",
            " a)                                                              ]']                              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50, 128)      16512       ['layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 50, 128)     527488      ['positional_encoding[1][0]',    \n",
            " eadAttention)                                                    'tf.math.logical_and[0][0]',    \n",
            "                                                                  'positional_encoding[1][0]']    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 50, 128)      0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 50, 128)      0           ['multi_head_attention_2[0][0]', \n",
            "                                                                  'positional_encoding[1][0]']    \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 50, 128)      0           ['dropout_1[0][0]',              \n",
            "                                                                  'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 50, 128)     256         ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 50, 128)     256         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 50, 128)     527488      ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'tf.__operators__.getitem[0][0]'\n",
            "                                                                 , 'layer_normalization_3[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 50, 128)      0           ['multi_head_attention_3[0][0]', \n",
            "                                                                  'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 50, 128)     256         ['add_5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 50, 128)      16512       ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 50, 128)      16512       ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 50, 128)      0           ['dense_3[0][0]',                \n",
            "                                                                  'layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 50, 128)     256         ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.math.logical_and_1 (TFOpLam  (None, 50, 50)      0           ['tf.__operators__.getitem_1[0][0\n",
            " bda)                                                            ]']                              \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 50, 128)     527488      ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'tf.math.logical_and_1[0][0]',  \n",
            "                                                                  'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 50, 128)      0           ['multi_head_attention_4[0][0]', \n",
            "                                                                  'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 50, 128)     256         ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 50, 128)     527488      ['layer_normalization_7[0][0]',  \n",
            " eadAttention)                                                    'tf.__operators__.getitem[0][0]'\n",
            "                                                                 , 'layer_normalization_3[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 50, 128)      0           ['multi_head_attention_5[0][0]', \n",
            "                                                                  'layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 50, 128)     256         ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 50, 128)      16512       ['layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50, 128)      16512       ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 50, 128)      0           ['dense_5[0][0]',                \n",
            "                                                                  'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 50, 128)     256         ['add_9[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 50, 1000)     129000      ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,651,560\n",
            "Trainable params: 3,651,560\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "1e24b41a"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6c0a34ca"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
        "             metrics=[\"accuracy\"] )"
      ],
      "id": "6c0a34ca"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b409e3-b109-4e2e-9af8-d2b7b45ae654",
        "id": "4njIV53Njz4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 8, 50, 50), dtype=tf.float32, name=None), name='multi_head_attention/softmax/Softmax:0', description=\"created by layer 'multi_head_attention'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 8, 50, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "attn_1 = encoder_attn_layers[0]\n",
        "print(attn_1)\n",
        "attn_1.shape"
      ],
      "id": "4njIV53Njz4e"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2315d96a",
        "outputId": "782ba9e0-b807-406f-bf17-320bba934940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 229s 64ms/step - loss: 2.7082 - accuracy: 0.4642 - val_loss: 1.8993 - val_accuracy: 0.5819\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 167s 53ms/step - loss: 1.7595 - accuracy: 0.6027 - val_loss: 1.5754 - val_accuracy: 0.6393\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 164s 53ms/step - loss: 1.5497 - accuracy: 0.6390 - val_loss: 1.4479 - val_accuracy: 0.6613\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 174s 56ms/step - loss: 1.4431 - accuracy: 0.6575 - val_loss: 1.3690 - val_accuracy: 0.6762\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 164s 52ms/step - loss: 1.3748 - accuracy: 0.6691 - val_loss: 1.3221 - val_accuracy: 0.6822\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 164s 52ms/step - loss: 1.3232 - accuracy: 0.6779 - val_loss: 1.2779 - val_accuracy: 0.6935\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 174s 56ms/step - loss: 1.2821 - accuracy: 0.6854 - val_loss: 1.2698 - val_accuracy: 0.6950\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 164s 53ms/step - loss: 1.2520 - accuracy: 0.6909 - val_loss: 1.2381 - val_accuracy: 0.7023\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 175s 56ms/step - loss: 1.2261 - accuracy: 0.6949 - val_loss: 1.2212 - val_accuracy: 0.7040\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 164s 53ms/step - loss: 1.1979 - accuracy: 0.7002 - val_loss: 1.2105 - val_accuracy: 0.7077\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbf38155c70>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.fit((X_train, X_train_dec), Y_train, epochs=10, \n",
        "         validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "id": "2315d96a"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5b3faa07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb2dd79-6c5c-4d0d-c41b-ccfac98c166e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 8, 50, 50), dtype=tf.float32, name=None), name='multi_head_attention/softmax/Softmax:0', description=\"created by layer 'multi_head_attention'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 8, 50, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "attn_1 = encoder_attn_layers[0]\n",
        "print(attn_1)\n",
        "attn_1.shape"
      ],
      "id": "5b3faa07"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}