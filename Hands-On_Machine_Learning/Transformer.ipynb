{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72b6affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
    "                               extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91846663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the english and spanish lists\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f815b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d858ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc58b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff242a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6913609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0\n",
    "        p, i = np.meshgrid(np.arange(max_length), \n",
    "                          2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** ( i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28fe163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63724834",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_units = 128\n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a41a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in # start with the encoded embeddings\n",
    "\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads = num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cd4c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_max_len_desc = max_length\n",
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "causal_mask = tf.linalg.band_part( # create lower triangluarl matrix\n",
    "    tf.ones((batch_max_len_desc, batch_max_len_desc), tf.bool), -1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7420051",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = Z # This is the result of the encoder layer\n",
    "Z = decoder_in # Now we start Z over for the decoder \n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask) \n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip=Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c112c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                      outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e24b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_3 (TextVect  (None, 50)          0           ['input_2[0][0]']                \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " text_vectorization_2 (TextVect  (None, 50)          0           ['input_1[0][0]']                \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 50, 128)      128000      ['text_vectorization_3[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.not_equal (TFOpLambda)  (None, 50)          0           ['text_vectorization_2[0][0]']   \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 50, 128)      128000      ['text_vectorization_2[0][0]']   \n",
      "                                                                                                  \n",
      " positional_encoding (Positiona  (None, 50, 128)     0           ['embedding[0][0]',              \n",
      " lEncoding)                                                       'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1, 50)       0           ['tf.math.not_equal[0][0]']      \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 50, 128)     527488      ['positional_encoding[0][0]',    \n",
      " dAttention)                                                      'tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'positional_encoding[0][0]']   \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 50, 128)      0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 50, 128)     256         ['add[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50, 128)      16512       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50, 128)      0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 50, 128)      0           ['dropout[0][0]',                \n",
      "                                                                  'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 50, 128)     256         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 50, 128)     527488      ['layer_normalization_1[0][0]',  \n",
      " eadAttention)                                                    'tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'layer_normalization_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.not_equal_1 (TFOpLambd  (None, 50)          0           ['text_vectorization_3[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 50, 128)      0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 1, 50)       0           ['tf.math.not_equal_1[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50, 128)     256         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.logical_and (TFOpLambd  (None, 50, 50)      0           ['tf.__operators__.getitem_1[0][0\n",
      " a)                                                              ]']                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50, 128)      16512       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 50, 128)     527488      ['positional_encoding[1][0]',    \n",
      " eadAttention)                                                    'tf.math.logical_and[0][0]',    \n",
      "                                                                  'positional_encoding[1][0]']    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 128)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 50, 128)      0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'positional_encoding[1][0]']    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 128)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 50, 128)     256         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50, 128)     256         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 50, 128)     527488      ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'layer_normalization_3[0][0]'] \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 50, 128)      0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 50, 128)     256         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 128)      16512       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50, 128)      16512       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 50, 128)      0           ['dense_3[0][0]',                \n",
      "                                                                  'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 50, 128)     256         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.logical_and_1 (TFOpLam  (None, 50, 50)      0           ['tf.__operators__.getitem_1[0][0\n",
      " bda)                                                            ]']                              \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 50, 128)     527488      ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'tf.math.logical_and_1[0][0]',  \n",
      "                                                                  'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 50, 128)      0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 50, 128)     256         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 50, 128)     527488      ['layer_normalization_7[0][0]',  \n",
      " eadAttention)                                                    'tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'layer_normalization_3[0][0]'] \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 50, 128)      0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 50, 128)     256         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50, 128)      16512       ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 50, 128)      16512       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 50, 128)      0           ['dense_5[0][0]',                \n",
      "                                                                  'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 50, 128)     256         ['add_9[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 50, 1000)     129000      ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,651,560\n",
      "Trainable params: 3,651,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b3faa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = TensorBoard(log_dir=\"logs\", histogram_freq=1, write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c0a34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
    "             metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Layer PositionalEncoding has arguments ['max_length', 'embed_size']\n",
      "in `__init__` and therefore must override `get_config()`.\n",
      "\n",
      "Example:\n",
      "\n",
      "class CustomLayer(keras.layers.Layer):\n",
      "    def __init__(self, arg1, arg2):\n",
      "        super().__init__()\n",
      "        self.arg1 = arg1\n",
      "        self.arg2 = arg2\n",
      "\n",
      "    def get_config(self):\n",
      "        config = super().get_config()\n",
      "        config.update({\n",
      "            \"arg1\": self.arg1,\n",
      "            \"arg2\": self.arg2,\n",
      "        })\n",
      "        return config\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py:1563: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 176/3125 [>.............................] - ETA: 39:19 - loss: 0.6528 - accuracy: 0.2332"
     ]
    }
   ],
   "source": [
    "model.fit((X_train, X_train_dec), Y_train, epochs=10, callbacks=[tb_callback],\n",
    "         validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
