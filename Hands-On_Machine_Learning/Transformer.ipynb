{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b6affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 10:26:03.522316: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
    "                               extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91846663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the english and spanish lists\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f815b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 10:26:12.699590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d858ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc58b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff242a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6913609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0\n",
    "        p, i = np.meshgrid(np.arange(max_length), \n",
    "                          2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** ( i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28fe163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63724834",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_units = 128\n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a41a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in # start with the encoded embeddings\n",
    "\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads = num_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "    )\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cd4c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_max_len_desc = max_length\n",
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "causal_mask = tf.linalg.band_part( # create lower triangluarl matrix\n",
    "    tf.ones((batch_max_len_desc, batch_max_len_desc), tf.bool), -1, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7420051",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = Z # This is the result of the encoder layer\n",
    "Z = decoder_in # Now we start Z over for the decoder \n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask) \n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip=Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03c112c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                      outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e24b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_1 (TextVect  (None, 50)          0           ['input_2[0][0]']                \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 50)          0           ['input_1[0][0]']                \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 50, 128)      128000      ['text_vectorization_1[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.not_equal_4 (TFOpLambd  (None, 50)          0           ['text_vectorization[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 50, 128)      128000      ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " positional_encoding (Positiona  (None, 50, 128)     0           ['embedding[0][0]',              \n",
      " lEncoding)                                                       'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 1, 50)       0           ['tf.math.not_equal_4[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 50, 128)     527488      ['positional_encoding[0][0]',    \n",
      " HeadAttention)                                                   'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 50, 128)      0           ['multi_head_attention_13[0][0]',\n",
      "                                                                  'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 50, 128)     256         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 50, 128)      16512       ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 50, 128)      0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 50, 128)      0           ['dropout_2[0][0]',              \n",
      "                                                                  'layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 50, 128)     256         ['add_20[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 50, 128)     527488      ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.not_equal_5 (TFOpLambd  (None, 50)          0           ['text_vectorization_1[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 50, 128)      0           ['multi_head_attention_14[0][0]',\n",
      "                                                                  'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 1, 50)       0           ['tf.math.not_equal_5[0][0]']    \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 50, 128)     256         ['add_21[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.logical_and_5 (TFOpLam  (None, 50, 50)      0           ['tf.__operators__.getitem_5[0][0\n",
      " bda)                                                            ]']                              \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 50, 128)      16512       ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 50, 128)     527488      ['positional_encoding[1][0]',    \n",
      " HeadAttention)                                                   'tf.math.logical_and_5[0][0]',  \n",
      "                                                                  'positional_encoding[1][0]']    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 50, 128)      0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 50, 128)      0           ['multi_head_attention_15[0][0]',\n",
      "                                                                  'positional_encoding[1][0]']    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 50, 128)      0           ['dropout_3[0][0]',              \n",
      "                                                                  'layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 50, 128)     256         ['add_23[0][0]']                 \n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 50, 128)     256         ['add_22[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (Multi  (None, 50, 128)     527488      ['layer_normalization_23[0][0]', \n",
      " HeadAttention)                                                   'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 50, 128)      0           ['multi_head_attention_16[0][0]',\n",
      "                                                                  'layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 50, 128)     256         ['add_24[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 50, 128)      16512       ['layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 50, 128)      16512       ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 50, 128)      0           ['dense_17[0][0]',               \n",
      "                                                                  'layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 50, 128)     256         ['add_25[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.logical_and_6 (TFOpLam  (None, 50, 50)      0           ['tf.__operators__.getitem_5[0][0\n",
      " bda)                                                            ]']                              \n",
      "                                                                                                  \n",
      " multi_head_attention_17 (Multi  (None, 50, 128)     527488      ['layer_normalization_25[0][0]', \n",
      " HeadAttention)                                                   'tf.math.logical_and_6[0][0]',  \n",
      "                                                                  'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 50, 128)      0           ['multi_head_attention_17[0][0]',\n",
      "                                                                  'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 50, 128)     256         ['add_26[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 50, 128)     527488      ['layer_normalization_26[0][0]', \n",
      " HeadAttention)                                                   'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 50, 128)      0           ['multi_head_attention_18[0][0]',\n",
      "                                                                  'layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 50, 128)     256         ['add_27[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 50, 128)      16512       ['layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 50, 128)      16512       ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 50, 128)      0           ['dense_19[0][0]',               \n",
      "                                                                  'layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 50, 128)     256         ['add_28[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 50, 1000)     129000      ['layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,651,560\n",
      "Trainable params: 3,651,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c0a34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "         validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3faa07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
