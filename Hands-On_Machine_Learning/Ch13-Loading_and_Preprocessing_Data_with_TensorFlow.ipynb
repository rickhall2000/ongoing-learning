{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a44f030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 07:47:44.218094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 07:47:50.808009: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fea8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcb36e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "X_nested = {\"a\": ([1,2,3], [4,5,6]), \"b\": [7,8,9]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce518ba4",
   "metadata": {},
   "source": [
    "### Chainging Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3297fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd8c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb1ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7602bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2): \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378cf4f",
   "metadata": {},
   "source": [
    "### Shuffling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7d1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f717d",
   "metadata": {},
   "source": [
    "### Interleaving Lines from Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a85499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    # this example uses variables that haven't been defined\n",
    "    # trying writing as a funciton so I don't have to comment out everything\n",
    "    filepath_dataset = tf.data.DataSet.list_files(train_filepaths, seed=42)\n",
    "    nreaders = 5\n",
    "    dataset = filepath_dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers)\n",
    "    \n",
    "    for line in dataset.take(5):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149f7ac",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b854dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    X_mean, X_std = [...]\n",
    "    n_inputs = 8\n",
    "    \n",
    "def parse_csv_line(line):\n",
    "    defs [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1])\n",
    "\n",
    "def preprocess(line):\n",
    "    x, y = parse_csv(line)\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1f66b",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288913fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n",
    "                      n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n",
    "                      batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprrocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefeatch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0234f4",
   "metadata": {},
   "source": [
    "### Using the Dataset with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66fa1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    train_set = csv_reader_dataset(train_filepaths)\n",
    "    valid_set = csv_reader_dataset(valid_filepaths)\n",
    "    test_set = csv_reader_dataset(test_filepaths)\n",
    "    \n",
    "    model = tf.keras.Sequential[...]\n",
    "    model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "    model.fit(train_set, validation_data=valid_set, epochs=5)\n",
    "    \n",
    "    test_mse = model.evaluate(test_set)\n",
    "    new_set = test_set.take(3)\n",
    "    y_pred = model.predict(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38fd7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradeints = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "def thunk():\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    loss_fn = tf.keras.losses.mean_squared_error\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
    "        train_one_epoch(model, optimizer, loss_fn,train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b272200",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d3be805",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f5a55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ca4f8",
   "metadata": {},
   "source": [
    "### Compressed TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3167baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"Compress, compress, compress!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f9d8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"], compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f9704",
   "metadata": {},
   "source": [
    "### TensorFlow Protobufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a7068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
    "                                                          b\"c@d.com\"]))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6bc73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    for _ in range(5):\n",
    "        f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bb600c",
   "metadata": {},
   "source": [
    "### Loading and Parsing Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8643c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc775fdb80>, 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc7797bbe0>, 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc7797bd00>, 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc7797bbe0>, 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n",
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc7797bc40>, 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n"
     ]
    }
   ],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse(serialized_example):\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).map(parse)\n",
    "for parsed_example in dataset:\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a8f29dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02e279d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example[\"emails\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3806cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc57d04dc0>, 'id': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([123, 123])>, 'name': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Alice', b'Alice'], dtype=object)>}\n",
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc779b3220>, 'id': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([123, 123])>, 'name': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Alice', b'Alice'], dtype=object)>}\n",
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fbc779b3430>, 'id': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([123])>, 'name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Alice'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "def parse(serialized_examples):\n",
    "    return tf.io.parse_example(serialized_examples, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(2).map(parse)\n",
    "for parsed_examples in dataset:\n",
    "    print(parsed_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fad0b5",
   "metadata": {},
   "source": [
    "## Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7a783",
   "metadata": {},
   "source": [
    "## The Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f3cd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    norm_layer = tf.keras.layers.Normalization()\n",
    "    model = tf.keras.models.Sequential([\n",
    "        norm_layer,\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "    norm_layer.adapt(X_train)\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3a49a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk_normalize_before_training():\n",
    "    norm_layer = tf.keras.layers.Normalization()\n",
    "    norm_layer.adapt(X_train)\n",
    "    X_train_scaled = norm_layer(X_train)\n",
    "    X_valid_scaled = norm_layer(X_valid)\n",
    "    \n",
    "    model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "    model.fit(X_train_scaled, y_train, epocs=5,\n",
    "             validation_data=(X_valid_scaled, y_valid)) \n",
    "    \n",
    "    final_model = tf.keras.Sequential([norm_layer, model])\n",
    "    X_new = X_test[:3]\n",
    "    y_pred = final_model(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "795342f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk_apply_nomrilzation_to_a_dataset():\n",
    "    dataset = dataset.map(lambda X, y: (norm_layer(X), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "192454f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also make your own layer\n",
    "import numpy as np\n",
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "    def adapt(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        return (inputs - self.mean_) / (self.std_ + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f488f8",
   "metadata": {},
   "source": [
    "### The Discretization Layer\n",
    "binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2609302e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
    "discreteize_layer = tf.keras.layers.Discretization(bin_boundaries=[18, 50])\n",
    "age_categories = discreteize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff8c815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357b337",
   "metadata": {},
   "source": [
    "### The CategoryEncoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "032791cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
    "onehot_layer(age_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d35681d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-hot encoding?\n",
    "two_age_categories = np.array([[1,0], [2,2], [2,0]])\n",
    "onehot_layer(two_age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61931293",
   "metadata": {},
   "source": [
    "### The StringLookup Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ade095af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = [\"Atlanta\", \"Paris\", \"Paris\", \"San Fransisco\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Atlanta\"], [\"Atlanta\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de7d35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fbc5b556310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Atlanta\"], [\"Atlanta\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2be8098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[5],\n",
       "       [7],\n",
       "       [4],\n",
       "       [3],\n",
       "       [4]])>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Atlanta\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ed070",
   "metadata": {},
   "source": [
    "### The Hashing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bbd99a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [3],\n",
       "       [1]])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n",
    "hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Atlanta\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c530b",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef542dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.02550352,  0.04228146],\n",
       "       [-0.02565923,  0.00688175],\n",
       "       [-0.02550352,  0.04228146]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
    "embedding_layer(np.array([2, 4, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a5e384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk_I_dont_get_why_this_doesnt_work():\n",
    "    tf.random.set_seed(42)\n",
    "    ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "    str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "    str_lookup_layer.adapt(ocean_prox)\n",
    "    lookup_and_embed = tf.keras.Sequential([\n",
    "        str_lookup_layer,\n",
    "        tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\n",
    "                                 output_dim=2)\n",
    "    ])\n",
    "\n",
    "    lookup_and_embed(np.array([[\"<1H OCEAN\"], [\"ISLAND\"], [\"<1H OCEAN\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d408d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    X_train_num, X_train_cat, y_train = [...]\n",
    "    X_valid_num, X_valid_cat, y_valid = [...]\n",
    "    \n",
    "    num_input = tf.keras.layers.Input(shape=8, name=\"num\")\n",
    "    cat_input = tf.keras.layers.input(shape=[], dytpe=tf.string, name=\"cat\")\n",
    "    cat_embeddings = lookup_and_embed(cat_input)\n",
    "    encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n",
    "    outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "    model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\n",
    "    model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "    history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,\n",
    "                       validation_data=((X_valid_num, X_valid_cat), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99af4a2",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28ee57c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
       "array([[2, 1, 0, 0],\n",
       "       [6, 2, 1, 2]])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e4222ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "array([[0.96725637, 0.6931472 , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.96725637, 1.3862944 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06901a2f",
   "metadata": {},
   "source": [
    "### Using Pretrained Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f78a262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_hub as hub\n",
    "#hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "#sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
    "#sentence_embeddings.numpy().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca92920",
   "metadata": {},
   "source": [
    "### Image Preprocessing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db772209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "images = load_sample_images()[\"images\"] \n",
    "crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)\n",
    "cropped_images = crop_image_layer(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02141a35",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0ef0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_datasets as tfds\n",
    "\n",
    "#datasets = tfds.load(name=mnist)\n",
    "#mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c2dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c068e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    mnist_train = mnist_train.shuffle(buffer_size=10_000, seed=42).batch(32)\n",
    "    mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "    mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1147dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    train_set, valid_set, test_set = tfs.load(\n",
    "        name=\"mnist\",\n",
    "        split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "        as_supervised=True\n",
    "    )\n",
    "    train_set = train_set.shuffle(buffer_size=10_000, seed=42).batch(32).prefetch(1)\n",
    "    valid_set = valid_set.batch(32).cache()\n",
    "    test_set = test_set.batch(32).cache()\n",
    "    tf.random.set_seed(42)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
    "    test_loss, test_accuracy = model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57466fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
