{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e1ae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 07:01:54.291606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 07:02:00.623293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "images = load_sample_images()[\"images\"]\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale = 1 / 255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206ce7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8d6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afe7bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301b8f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\")\n",
    "fmaps = conv_layer(images)\n",
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77da1aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 7, 3, 32), (32,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels, biases = conv_layer.get_weights()\n",
    "kernels.shape, biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c052bea",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad6add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a09070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        groups = shape[-1]\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31490dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a614826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = tf.keras.layers.Lambda(\n",
    "    lambda X: tf.reduce_mean(X, axis=[1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf1cf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.64338624, 0.5971759 , 0.5824972 ],\n",
       "       [0.76306933, 0.26011038, 0.10849128]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_pool(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c979c",
   "metadata": {},
   "source": [
    "## CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a1eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
    "                       activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28,28,1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548688d8",
   "metadata": {},
   "source": [
    "## Implementing a ResNet34 CNN Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c0d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                       padding=\"same\", kernel_initializer=\"he_normal\", \n",
    "                       use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "            skip_Z = inputs\n",
    "            for layer in self.skip_layers:\n",
    "                skip_Z = layer(skip_Z)\n",
    "            return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "798e9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "    \n",
    "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083f02d",
   "metadata": {},
   "source": [
    "## Using Pretrained Models from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8f05e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102967424/102967424 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.applications.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "354485ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_sample_images()[\"images\"]\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224,\n",
    "                                         crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0a0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0efaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = model.predict(inputs)\n",
    "Y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daa969eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #0\n",
      "  n03877845 - palace       54.69%\n",
      "  n03781244 - monastery    24.72%\n",
      "  n02825657 - bell_cote    18.55%\n",
      "Image #1\n",
      "  n04522168 - vase         32.66%\n",
      "  n11939491 - daisy        17.81%\n",
      "  n03530642 - honeycomb    12.06%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(f\"Image #{image_index}\")\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f\"  {class_id} - {name:12s} {y_proba:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611b6e2",
   "metadata": {},
   "source": [
    "## Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79493e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39b9b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "370fce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1b3bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8d53797",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                    include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2f5f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f667380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "86/86 [==============================] - 236s 3s/step - loss: 0.7131 - accuracy: 0.8081 - val_loss: 0.6584 - val_accuracy: 0.8494\n",
      "Epoch 2/3\n",
      "86/86 [==============================] - 235s 3s/step - loss: 0.3175 - accuracy: 0.9102 - val_loss: 0.5240 - val_accuracy: 0.8748\n",
      "Epoch 3/3\n",
      "86/86 [==============================] - 232s 3s/step - loss: 0.2151 - accuracy: 0.9317 - val_loss: 0.5541 - val_accuracy: 0.8711\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6ec9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8368839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "86/86 [==============================] - 465s 5s/step - loss: 0.5177 - accuracy: 0.8481 - val_loss: 7.1437 - val_accuracy: 0.5826\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 459s 5s/step - loss: 0.2227 - accuracy: 0.9230 - val_loss: 0.6866 - val_accuracy: 0.8439\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 433s 5s/step - loss: 0.1396 - accuracy: 0.9578 - val_loss: 0.4185 - val_accuracy: 0.8584\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 427s 5s/step - loss: 0.0615 - accuracy: 0.9789 - val_loss: 0.3619 - val_accuracy: 0.8947\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 432s 5s/step - loss: 0.0435 - accuracy: 0.9866 - val_loss: 0.6185 - val_accuracy: 0.8584\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 427s 5s/step - loss: 0.0368 - accuracy: 0.9876 - val_loss: 0.4841 - val_accuracy: 0.8966\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 427s 5s/step - loss: 0.0457 - accuracy: 0.9855 - val_loss: 0.4764 - val_accuracy: 0.8748\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - 429s 5s/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 0.5221 - val_accuracy: 0.8784\n",
      "Epoch 9/10\n",
      "86/86 [==============================] - 425s 5s/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 0.3764 - val_accuracy: 0.9147\n",
      "Epoch 10/10\n",
      "86/86 [==============================] - 425s 5s/step - loss: 0.0134 - accuracy: 0.9945 - val_loss: 0.5948 - val_accuracy: 0.8947\n"
     ]
    }
   ],
   "source": [
    "optimzer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723638bb",
   "metadata": {},
   "source": [
    "## Classification and Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720a4622",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mxception\u001b[38;5;241m.\u001b[39mXCeption(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                                      include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m avg \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling2D()(base_model\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m      4\u001b[0m class_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(n_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(avg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.xception.XCeption(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = tf.keras.layers.Dense(4)(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input,\n",
    "                      outputs=[class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "             loss_weights=[0.8, 0.2],\n",
    "             optimizer=optimzer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb9974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
