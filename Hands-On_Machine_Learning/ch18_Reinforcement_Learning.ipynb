{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a99fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe9d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9f978cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78813e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffc7c8d4c70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93c40d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3efb5599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "535f864d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40602b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "877e4ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78b06d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "541ac499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "477b1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.698, 8.389445512070509, 24.0, 63.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(totals), np.std(totals), min(totals), max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405763d1",
   "metadata": {},
   "source": [
    "## Neural Network Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8499de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 08:36:18.070617: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-27 08:36:29.660033: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09831570",
   "metadata": {},
   "source": [
    "## Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3dfa359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1,1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0584ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(\n",
    "                env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    \n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed0ece7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) -2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) \n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee46ecaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41e5358f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10,20]],\n",
    "                              discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c406a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5723b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36c4f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "            for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41d52b",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "950f7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [  # shape=[s, a, s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]\n",
    "]\n",
    "rewards = [  # shape=[s, a, s']\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
    "]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48ffbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15e4ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp]\n",
    "                * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
    "                for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f5ea389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b821850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20d039",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "513e25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6f5658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efe51ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 0.05\n",
    "decay = 0.005\n",
    "gamma = 0.90\n",
    "state = 0\n",
    "\n",
    "for iteration in range(10_000):\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = Q_values[next_state].max()\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2256e",
   "metadata": {},
   "source": [
    "## Implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "085e5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [4]\n",
    "n_outputs = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e03b67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return Q_values.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d96ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d17aef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    return [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(6)\n",
    "    ] # [states, actions, rewards, next_states, dones, truncateds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80c1462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    return next_state, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cfbca348",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, ations, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = model.predict(next_states, verbose=0)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    runs = 1 - (dones | truncateds)\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ddc64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(600):\n",
    "    obs, info = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "            \n",
    "    if episode > 50:\n",
    "        training_step(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a375a8",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10413b",
   "metadata": {},
   "source": [
    "### Fixed Q-value Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ac1ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a226e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updates_from_above():\n",
    "    next_Q_values = target.predict(next_states, verbose=0)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e9fef",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2853ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = model.predict(next_states, verbose=0)\n",
    "    best_next_actions = next_Q_values.argmax(axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask\n",
    "                        ).sum(axis=1)\n",
    "    [...] # then same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44ca36",
   "metadata": {},
   "source": [
    "### Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c2423e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_states = tf.keras.layers.Input(shape=[4])\n",
    "hidden1 = tf.keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
    "hidden2 = tf.keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
    "state_values = tf.keras.layers.Dense(1)(hidden2)\n",
    "raw_advantages = tf.keras.layers.Dense(n_outputs)(hidden1)\n",
    "advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1,\n",
    "                                           keepdims=True)\n",
    "Q_values = state_values + advantages\n",
    "model = tf.keras.Model(inputs=[input_states], outputs=[Q_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21238c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
